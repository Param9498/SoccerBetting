{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e26c3b2b",
   "metadata": {},
   "source": [
    "# Scraper Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3dcba49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_urls import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5edbd8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "# tune with line_profiler\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "399d9fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with EPL 2019-2020\n",
    "urls = scoresfixtures(\"https://fbref.com/en/comps/9/3232/schedule/2019-2020-Premier-League-Scores-and-Fixtures\", \"div_sched_3232_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d09d440",
   "metadata": {},
   "source": [
    "## Original Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ba2b88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(urls):\n",
    "    '''\n",
    "    Description: This function goes to de URL of the match and treat all data in order to append it in one single Dataframe.\n",
    "    \n",
    "    Input:\n",
    "        - urls: urls of all the match reports\n",
    "        - league: league\n",
    "        - season: season\n",
    "    Output:\n",
    "        - Dataframe treated from the match saved on my machine excel file\n",
    "    \n",
    "    '''\n",
    "    df_all = pd.DataFrame()\n",
    "    ind = 0\n",
    "    \n",
    "    for url in urls:    \n",
    "        # make the request\n",
    "        pg = 'https://fbref.com'\n",
    "        url_pg = pg+ url\n",
    "        req = requests.get(url_pg)\n",
    "        if req.status_code == 200:\n",
    "            content = req.content\n",
    "        # accessing data from site\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        \n",
    "        table_geral = soup.find_all(class_ = \"table_wrapper tabbed\")\n",
    "        table_1 = table_geral[0]\n",
    "        table_2 = table_geral[1]\n",
    "        table_3 = soup.find(class_='venuetime')\n",
    "\n",
    "        \n",
    "        # possession\n",
    "        percentage = str(soup.find_all('strong')).split('%')\n",
    "        possession_1 = int(percentage[0][-2:])\n",
    "        possession_2 = int(percentage[1][-2:])\n",
    "        \n",
    "        # save percentage\n",
    "        save = str(soup.find_all('td')).split(\"save_pct\")\n",
    "        save1 = save[1].split('>')[1].split('<')[0]\n",
    "        save2 = save[2].split('>')[1].split('<')[0]\n",
    "        save_1 = float(save1) if save1 != '' else 0\n",
    "        save_2 = float(save2) if save2 != '' else 0\n",
    "     \n",
    "\n",
    "\n",
    "        #collecting data\n",
    "        date = table_3.get('data-venue-date')        \n",
    "\n",
    "\n",
    "        #treating data\n",
    "        match = str(soup.title)\n",
    "        match = match.replace(\" \",\"_\")\n",
    "        match = match.replace(\"<title>\",\"\")\n",
    "        match = match.replace(\".\",\"\")\n",
    "        match_final = match.split(\"Report\")[0]\n",
    "\n",
    "\n",
    "        #treating data\n",
    "        match_final = match_final.split(\"_Match\")\n",
    "        match_final = match_final[0]    \n",
    "\n",
    "\n",
    "        # STR transform and reading tables\n",
    "        table_str_1 = str(table_1)\n",
    "        table_str_2 = str(table_2)\n",
    "        df_1 = pd.read_html(table_str_1)[0]\n",
    "        df_2 = pd.read_html(table_str_2)[0]\n",
    "\n",
    "        #treating data\n",
    "        team = str(match_final)\n",
    "        team = team.replace(\"_\",\" \")\n",
    "        team = team.split(\" vs \")\n",
    "        team_1 = str(team[0])\n",
    "        team_2 = str(team[1])\n",
    "\n",
    "        \n",
    "        #Dtframe transforming\n",
    "        df_1 = pd.DataFrame(df_1).tail(1)      #use only the summary row\n",
    "        df_1.columns = df_1.columns.droplevel()\n",
    "        df_1['Team'] = str(team_1)\n",
    "        df_1['Home or Away'] = 'Home'\n",
    "        df_1['Match'] = str(match_final)\n",
    "        df_1['Date'] = str(date)\n",
    "        df_1['Possession'] = possession_1\n",
    "        df_1['Save%'] = save_1\n",
    "  \n",
    "\n",
    "        df_2 = pd.DataFrame(df_2).tail(1)      #use only the summary row\n",
    "        df_2.columns = df_2.columns.droplevel()\n",
    "        df_2['Team'] = str(team_2)\n",
    "        df_2['Home or Away'] = 'Away'\n",
    "        df_2['Match'] = str(match_final)\n",
    "        df_2['Date'] = str(date)\n",
    "        df_2['Possession'] = possession_2\n",
    "        df_2['Save%'] = save_2\n",
    "\n",
    "        \n",
    "        df_all = df_all.append(df_1).append(df_2)\n",
    "        \n",
    "        ind += 1\n",
    "        if ind%50 == 0:\n",
    "            print('scraped %d matches'%ind)\n",
    "    \n",
    "    df_all.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b89a8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraped 50 matches\n",
      "scraped 100 matches\n",
      "scraped 150 matches\n",
      "scraped 200 matches\n",
      "scraped 250 matches\n",
      "scraped 300 matches\n",
      "scraped 350 matches\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-07 s\n",
       "\n",
       "Total time: 815.18 s\n",
       "File: <ipython-input-4-415dae2f7cd4>\n",
       "Function: get_df at line 1\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     1                                           def get_df(urls):\n",
       "     2                                               '''\n",
       "     3                                               Description: This function goes to de URL of the match and treat all data in order to append it in one single Dataframe.\n",
       "     4                                               \n",
       "     5                                               Input:\n",
       "     6                                                   - urls: urls of all the match reports\n",
       "     7                                                   - league: league\n",
       "     8                                                   - season: season\n",
       "     9                                               Output:\n",
       "    10                                                   - Dataframe treated from the match saved on my machine excel file\n",
       "    11                                               \n",
       "    12                                               '''\n",
       "    13         1       5895.0   5895.0      0.0      df_all = pd.DataFrame()\n",
       "    14         1         11.0     11.0      0.0      ind = 0\n",
       "    15                                               \n",
       "    16       381       3500.0      9.2      0.0      for url in urls:    \n",
       "    17                                                   # make the request\n",
       "    18       380       2152.0      5.7      0.0          pg = 'https://fbref.com'\n",
       "    19       380       3667.0      9.7      0.0          url_pg = pg+ url\n",
       "    20       380 1015519715.0 2672420.3     12.5          req = requests.get(url_pg)\n",
       "    21       380       9102.0     24.0      0.0          if req.status_code == 200:\n",
       "    22       380      33529.0     88.2      0.0              content = req.content\n",
       "    23                                                   # accessing data from site\n",
       "    24       380 3282314181.0 8637668.9     40.3          soup = BeautifulSoup(content, 'html.parser')\n",
       "    25                                           \n",
       "    26                                                   \n",
       "    27       380  832653008.0 2191192.1     10.2          table_geral = soup.find_all(class_ = \"table_wrapper tabbed\")\n",
       "    28       380      13471.0     35.5      0.0          table_1 = table_geral[0]\n",
       "    29       380       3027.0      8.0      0.0          table_2 = table_geral[1]\n",
       "    30       380   18289752.0  48130.9      0.2          table_3 = soup.find(class_='venuetime')\n",
       "    31                                           \n",
       "    32                                                   \n",
       "    33                                                   # possession\n",
       "    34       380   83314715.0 219249.2      1.0          percentage = str(soup.find_all('strong')).split('%')\n",
       "    35       380      16289.0     42.9      0.0          possession_1 = int(percentage[0][-2:])\n",
       "    36       380       4190.0     11.0      0.0          possession_2 = int(percentage[1][-2:])\n",
       "    37                                                   \n",
       "    38                                                   # save percentage\n",
       "    39       380  926831406.0 2439030.0     11.4          save = str(soup.find_all('td')).split(\"save_pct\")\n",
       "    40       380    1400670.0   3686.0      0.0          save1 = save[1].split('>')[1].split('<')[0]\n",
       "    41       380     419207.0   1103.2      0.0          save2 = save[2].split('>')[1].split('<')[0]\n",
       "    42       380      18082.0     47.6      0.0          save_1 = float(save1) if save1 != '' else 0\n",
       "    43       380       4993.0     13.1      0.0          save_2 = float(save2) if save2 != '' else 0\n",
       "    44                                                \n",
       "    45                                           \n",
       "    46                                           \n",
       "    47                                                   #collecting data\n",
       "    48       380      12750.0     33.6      0.0          date = table_3.get('data-venue-date')        \n",
       "    49                                           \n",
       "    50                                           \n",
       "    51                                                   #treating data\n",
       "    52       380     683218.0   1797.9      0.0          match = str(soup.title)\n",
       "    53       380       9700.0     25.5      0.0          match = match.replace(\" \",\"_\")\n",
       "    54       380       8153.0     21.5      0.0          match = match.replace(\"<title>\",\"\")\n",
       "    55       380       5288.0     13.9      0.0          match = match.replace(\".\",\"\")\n",
       "    56       380       6076.0     16.0      0.0          match_final = match.split(\"Report\")[0]\n",
       "    57                                           \n",
       "    58                                           \n",
       "    59                                                   #treating data\n",
       "    60       380       3863.0     10.2      0.0          match_final = match_final.split(\"_Match\")\n",
       "    61       380       2531.0      6.7      0.0          match_final = match_final[0]    \n",
       "    62                                           \n",
       "    63                                           \n",
       "    64                                                   # STR transform and reading tables\n",
       "    65       380  463447310.0 1219598.2      5.7          table_str_1 = str(table_1)\n",
       "    66       380  464262214.0 1221742.7      5.7          table_str_2 = str(table_2)\n",
       "    67       380  497284419.0 1308643.2      6.1          df_1 = pd.read_html(table_str_1)[0]\n",
       "    68       380  497501552.0 1309214.6      6.1          df_2 = pd.read_html(table_str_2)[0]\n",
       "    69                                           \n",
       "    70                                                   #treating data\n",
       "    71       380      11874.0     31.2      0.0          team = str(match_final)\n",
       "    72       380       7812.0     20.6      0.0          team = team.replace(\"_\",\" \")\n",
       "    73       380       5771.0     15.2      0.0          team = team.split(\" vs \")\n",
       "    74       380       3360.0      8.8      0.0          team_1 = str(team[0])\n",
       "    75       380       2872.0      7.6      0.0          team_2 = str(team[1])\n",
       "    76                                           \n",
       "    77                                                   \n",
       "    78                                                   #Dtframe transforming\n",
       "    79       380     491632.0   1293.8      0.0          df_1 = pd.DataFrame(df_1).tail(1)      #use only the summary row\n",
       "    80       380     274961.0    723.6      0.0          df_1.columns = df_1.columns.droplevel()\n",
       "    81       380    2469304.0   6498.2      0.0          df_1['Team'] = str(team_1)\n",
       "    82       380    1981825.0   5215.3      0.0          df_1['Home or Away'] = 'Home'\n",
       "    83       380    1928852.0   5075.9      0.0          df_1['Match'] = str(match_final)\n",
       "    84       380    1904341.0   5011.4      0.0          df_1['Date'] = str(date)\n",
       "    85       380    1959217.0   5155.8      0.0          df_1['Possession'] = possession_1\n",
       "    86       380    1909917.0   5026.1      0.0          df_1['Save%'] = save_1\n",
       "    87                                             \n",
       "    88                                           \n",
       "    89       380     421001.0   1107.9      0.0          df_2 = pd.DataFrame(df_2).tail(1)      #use only the summary row\n",
       "    90       380     263354.0    693.0      0.0          df_2.columns = df_2.columns.droplevel()\n",
       "    91       380    2174185.0   5721.5      0.0          df_2['Team'] = str(team_2)\n",
       "    92       380    1947792.0   5125.8      0.0          df_2['Home or Away'] = 'Away'\n",
       "    93       380    1902493.0   5006.6      0.0          df_2['Match'] = str(match_final)\n",
       "    94       380    1900903.0   5002.4      0.0          df_2['Date'] = str(date)\n",
       "    95       380    1930728.0   5080.9      0.0          df_2['Possession'] = possession_2\n",
       "    96       380    1906168.0   5016.2      0.0          df_2['Save%'] = save_2\n",
       "    97                                           \n",
       "    98                                                   \n",
       "    99       380   42297200.0 111308.4      0.5          df_all = df_all.append(df_1).append(df_2)\n",
       "   100                                                   \n",
       "   101       380       5769.0     15.2      0.0          ind += 1\n",
       "   102       380       2880.0      7.6      0.0          if ind%50 == 0:\n",
       "   103         7       8021.0   1145.9      0.0              print('scraped %d matches'%ind)\n",
       "   104                                               \n",
       "   105         1        474.0    474.0      0.0      df_all.reset_index(drop=True, inplace=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f get_df get_df(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cee51f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the runtime for the original version\n",
    "time_original = 815.18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d24d89",
   "metadata": {},
   "source": [
    "## Advanced Python Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b579b1",
   "metadata": {},
   "source": [
    "### 1. Multi-threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d40e09df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from multiprocessing.pool import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a792c138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for scraping: 137.5474328994751 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=8) as ex:\n",
    "    ex.map(get_df, urls)\n",
    "\n",
    "time_threading = time.time() - start\n",
    "print('Time for scraping: {} seconds'.format(time_threading))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c2d65",
   "metadata": {},
   "source": [
    "### 2. Cython with Predefined Ctype Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8bbc9493",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "093ae2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Cython.Compiler.Options import get_directive_defaults\n",
    "\n",
    "directive_defaults = get_directive_defaults()\n",
    "directive_defaults['linetrace'] = True\n",
    "directive_defaults['binding'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3be523d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython -f \n",
    "# cython: linetrace=True\n",
    "# cython: binding=True\n",
    "# distutils: define_macros=CYTHON_TRACE_NOGIL=1\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import xlsxwriter\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def get_df_cython(urls):\n",
    "    '''\n",
    "    Description: This function goes to de URL of the match and treat all data in order to append it in one single Dataframe.\n",
    "    \n",
    "    Input:\n",
    "        - urls: urls of all the match reports\n",
    "        - league: league\n",
    "        - season: season\n",
    "    Output:\n",
    "        - Dataframe treated from the match saved on my machine excel file\n",
    "    \n",
    "    '''\n",
    "    df_all = pd.DataFrame()\n",
    "    ind = 0\n",
    "    \n",
    "    # pre-define ctype variables\n",
    "    cdef list percentage, save\n",
    "    cdef str match, table_str_1, table_str_2, team_1, team_2\n",
    "    cdef int possession_1, possession_2\n",
    "    cdef float save_1, save_2\n",
    "\n",
    "\n",
    "    \n",
    "    for url in urls:    \n",
    "        # make the request\n",
    "        pg = 'https://fbref.com'\n",
    "        url_pg = pg+ url\n",
    "        req = requests.get(url_pg)\n",
    "        if req.status_code == 200:\n",
    "            content = req.content\n",
    "        # accessing data from site\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        \n",
    "        table_geral = soup.find_all(class_ = \"table_wrapper tabbed\")\n",
    "        table_1 = table_geral[0]\n",
    "        table_2 = table_geral[1]\n",
    "        table_3 = soup.find(class_='venuetime')\n",
    "\n",
    "        \n",
    "        # possession\n",
    "        percentage = str(soup.find_all('strong')).split('%')\n",
    "        possession_1 = int(percentage[0][-2:])\n",
    "        possession_2 = int(percentage[1][-2:])\n",
    "        \n",
    "        # save percentage\n",
    "        save = str(soup.find_all('td')).split(\"save_pct\")\n",
    "        save1 = save[1].split('>')[1].split('<')[0]\n",
    "        save2 = save[2].split('>')[1].split('<')[0]\n",
    "        save_1 = float(save1) if save1 != '' else 0\n",
    "        save_2 = float(save2) if save2 != '' else 0\n",
    "     \n",
    "\n",
    "\n",
    "        #collecting data\n",
    "        date = table_3.get('data-venue-date')        \n",
    "\n",
    "\n",
    "        #treating data\n",
    "        match = str(soup.title)\n",
    "        match = match.replace(\" \",\"_\")\n",
    "        match = match.replace(\"<title>\",\"\")\n",
    "        match = match.replace(\".\",\"\")\n",
    "        match_final = match.split(\"Report\")[0]\n",
    "\n",
    "\n",
    "        #treating data\n",
    "        match_final = match_final.split(\"_Match\")\n",
    "        match_final = match_final[0]    \n",
    "\n",
    "\n",
    "        # STR transform and reading tables\n",
    "        table_str_1 = str(table_1)\n",
    "        table_str_2 = str(table_2)\n",
    "        df_1 = pd.read_html(table_str_1)[0]\n",
    "        df_2 = pd.read_html(table_str_2)[0]\n",
    "\n",
    "        #treating data\n",
    "        team = str(match_final)\n",
    "        team = team.replace(\"_\",\" \")\n",
    "        team = team.split(\" vs \")\n",
    "        team_1 = str(team[0])\n",
    "        team_2 = str(team[1])\n",
    "\n",
    "        \n",
    "        #Dtframe transforming\n",
    "        df_1 = pd.DataFrame(df_1).tail(1)      #use only the summary row\n",
    "        df_1.columns = df_1.columns.droplevel()\n",
    "        df_1['Team'] = str(team_1)\n",
    "        df_1['Home or Away'] = 'Home'\n",
    "        df_1['Match'] = str(match_final)\n",
    "        df_1['Date'] = str(date)\n",
    "        df_1['Possession'] = possession_1\n",
    "        df_1['Save%'] = save_1\n",
    "  \n",
    "\n",
    "        df_2 = pd.DataFrame(df_2).tail(1)      #use only the summary row\n",
    "        df_2.columns = df_2.columns.droplevel()\n",
    "        df_2['Team'] = str(team_2)\n",
    "        df_2['Home or Away'] = 'Away'\n",
    "        df_2['Match'] = str(match_final)\n",
    "        df_2['Date'] = str(date)\n",
    "        df_2['Possession'] = possession_2\n",
    "        df_2['Save%'] = save_2\n",
    "\n",
    "        \n",
    "        df_all = df_all.append(df_1).append(df_2)\n",
    "        \n",
    "        ind += 1\n",
    "        if ind%50 == 0:\n",
    "            print('scraped %d matches'%ind)\n",
    "        \n",
    "    df_all.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0c570af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraped 50 matches\n",
      "scraped 100 matches\n",
      "scraped 150 matches\n",
      "scraped 200 matches\n",
      "scraped 250 matches\n",
      "scraped 300 matches\n",
      "scraped 350 matches\n",
      "Time for scraping: 425.0426309108734 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "get_df_cython(urls)\n",
    "\n",
    "time_cython = time.time() - start\n",
    "print('Time for scraping: {} seconds'.format(time_cython))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2017936",
   "metadata": {},
   "source": [
    "### 3. Parallel Computing with Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "509f1fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numba\n",
    "from numba import cuda, jit\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5f8aeed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(parallel = True)\n",
    "def get_df_numba(urls):\n",
    "    '''\n",
    "    Description: This function goes to de URL of the match and treat all data in order to append it in one single Dataframe.\n",
    "    \n",
    "    Input:\n",
    "        - urls: urls of all the match reports\n",
    "        - league: league\n",
    "        - season: season\n",
    "    Output:\n",
    "        - Dataframe treated from the match saved on my machine excel file\n",
    "    \n",
    "    '''\n",
    "    df_all = pd.DataFrame()\n",
    "    ind = 0\n",
    "    \n",
    "    for url in urls:    \n",
    "        # make the request\n",
    "        pg = 'https://fbref.com'\n",
    "        url_pg = pg+ url\n",
    "        req = requests.get(url_pg)\n",
    "        if req.status_code == 200:\n",
    "            content = req.content\n",
    "        # accessing data from site\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        \n",
    "        table_geral = soup.find_all(class_ = \"table_wrapper tabbed\")\n",
    "        table_1 = table_geral[0]\n",
    "        table_2 = table_geral[1]\n",
    "        table_3 = soup.find(class_='venuetime')\n",
    "\n",
    "        \n",
    "        # possession\n",
    "        percentage = str(soup.find_all('strong')).split('%')\n",
    "        possession_1 = int(percentage[0][-2:])\n",
    "        possession_2 = int(percentage[1][-2:])\n",
    "        \n",
    "        # save percentage\n",
    "        save = str(soup.find_all('td')).split(\"save_pct\")\n",
    "        save1 = save[1].split('>')[1].split('<')[0]\n",
    "        save2 = save[2].split('>')[1].split('<')[0]\n",
    "        save_1 = float(save1) if save1 != '' else 0\n",
    "        save_2 = float(save2) if save2 != '' else 0\n",
    "     \n",
    "\n",
    "\n",
    "        #collecting data\n",
    "        date = table_3.get('data-venue-date')        \n",
    "\n",
    "\n",
    "        #treating data\n",
    "        match = str(soup.title)\n",
    "        match = match.replace(\" \",\"_\")\n",
    "        match = match.replace(\"<title>\",\"\")\n",
    "        match = match.replace(\".\",\"\")\n",
    "        match_final = match.split(\"Report\")[0]\n",
    "\n",
    "\n",
    "        #treating data\n",
    "        match_final = match_final.split(\"_Match\")\n",
    "        match_final = match_final[0]    \n",
    "\n",
    "\n",
    "        # STR transform and reading tables\n",
    "        table_str_1 = str(table_1)\n",
    "        table_str_2 = str(table_2)\n",
    "        df_1 = pd.read_html(table_str_1)[0]\n",
    "        df_2 = pd.read_html(table_str_2)[0]\n",
    "\n",
    "        #treating data\n",
    "        team = str(match_final)\n",
    "        team = team.replace(\"_\",\" \")\n",
    "        team = team.split(\" vs \")\n",
    "        team_1 = str(team[0])\n",
    "        team_2 = str(team[1])\n",
    "\n",
    "        \n",
    "        #Dtframe transforming\n",
    "        df_1 = pd.DataFrame(df_1).tail(1)      #use only the summary row\n",
    "        df_1.columns = df_1.columns.droplevel()\n",
    "        df_1['Team'] = str(team_1)\n",
    "        df_1['Home or Away'] = 'Home'\n",
    "        df_1['Match'] = str(match_final)\n",
    "        df_1['Date'] = str(date)\n",
    "        df_1['Possession'] = possession_1\n",
    "        df_1['Save%'] = save_1\n",
    "  \n",
    "\n",
    "        df_2 = pd.DataFrame(df_2).tail(1)      #use only the summary row\n",
    "        df_2.columns = df_2.columns.droplevel()\n",
    "        df_2['Team'] = str(team_2)\n",
    "        df_2['Home or Away'] = 'Away'\n",
    "        df_2['Match'] = str(match_final)\n",
    "        df_2['Date'] = str(date)\n",
    "        df_2['Possession'] = possession_2\n",
    "        df_2['Save%'] = save_2\n",
    "\n",
    "        \n",
    "        df_all = df_all.append(df_1).append(df_2)\n",
    "        \n",
    "        ind += 1\n",
    "        if ind%50 == 0:\n",
    "            print('scraped %d matches'%ind)\n",
    "    \n",
    "    df_all.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "35434c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraped 50 matches\n",
      "scraped 100 matches\n",
      "scraped 150 matches\n",
      "scraped 200 matches\n",
      "scraped 250 matches\n",
      "scraped 300 matches\n",
      "scraped 350 matches\n",
      "Time for scraping: 364.4957365989685 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "get_df_numba(urls)\n",
    "\n",
    "time_numba = time.time() - start\n",
    "print('Time for scraping: {} seconds'.format(time_numba))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6368206",
   "metadata": {},
   "source": [
    "## Accaceleration for Each Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "89175cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed up for multi-threading: 592 %\n",
      "speed up for cython: 191 %\n",
      "speed up for numba: 223 %\n"
     ]
    }
   ],
   "source": [
    "acc_threading = time_original/time_threading * 100\n",
    "acc_cython = time_original/time_cython * 100\n",
    "acc_numba = time_original/time_numba * 100\n",
    "\n",
    "print(f'speed up for multi-threading: {int(acc_threading)} %')\n",
    "print(f'speed up for cython: {int(acc_cython)} %')\n",
    "print(f'speed up for numba: {int(acc_numba)} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
